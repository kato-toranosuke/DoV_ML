import numpy as np
import pandas as pd
from typing import List, Any, Union, Dict
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
import pprint
from sklearn.ensemble import ExtraTreesClassifier
import joblib
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix, f1_score


CSV_PATH = "../out/csv/"


def CsvToDf(csv_list: List) -> pd.DataFrame:
    df = pd.concat([pd.read_csv(CSV_PATH + filename) for filename in csv_list])
    return df


# 読み込むCSVファイルのリスト
csv_list = ["features_ch0.csv"]
df = CsvToDf(csv_list)


# session1のデータの抽出(式を評価するengineとしてnumexprを使用することで、処理の高速化を狙う。)
train_set = df.query('session_id == "trial1"', engine='numexpr')
test_set = df.query('session_id == "trial2"', engine='numexpr')


class DataFrameSelector(BaseEstimator, TransformerMixin):
    '''
    DataFrameから、属性を選択し、Numpy Arrayへ変換する変換器。
    '''

    def __init__(self, attribute_names: List) -> None:
        self.attribute_names = attribute_names

    def fit(self, X: pd.DataFrame, y=None):
        return self

    def transform(self, X: pd.DataFrame) -> np.ndarray:
        return X[self.attribute_names].to_numpy()


class ValueTransducer(BaseEstimator, TransformerMixin):
    '''
    値を他の値に変換する。
    '''

    def __init__(self, vals) -> None:
        self.vals = vals

    def fit(self, X: np.ndarray, y=None):
        return self

    def transform(self, X: np.ndarray) -> np.ndarray:
        X_ = []
        # Trueになる値->1, それ以外->0
        for val in self.vals:
            x_ = np.where(X == val, 1, 0)
            X_.append(x_)
        # ndarrayに変換
        X_ = np.array(X_)

        # 1次元配列にまとめる
        label_list = []
        for i in range(len(X)):
            bool = X_[:, i].any()
            label_list.append(int(bool))
        # ndarrayに変換
        label_np = np.array(label_list)

        return label_np


# 特徴量に該当する列の抽出
feature_attribs = ['low_power', 'high_power', 'hlbr', 'coe1[0]',	'coe1[1]', 'coe3[0]', 'coe3[1]', 'coe3[2]',	'coe3[3]', 'ratio_max_to_10ms_ave_peaks', 'ratio_max_to_9th_ave_peaks', 'ac_std', 'ac_auc', 'diff_std', 'diff_auc',	'srmr', 'gp_max_val_std', 'gp_max_val_range',
                       'gp_max_val_min', 'gp_max_val_max', 'gp_max_val_mean', 'gp_max_ix_std', 'gp_max_ix_range', 'gp_max_ix_min', 'gp_max_ix_max', 'gp_max_ix_mean', 'gp_auc_std', 'gp_auc_range', 'gp_auc_min', 'gp_auc_max', 'gp_auc_mean',	'tdoa_std', 'tdoa_range', 'tdoa_min', 'tdoa_max', 'tdoa_mean']


# pipeline
features_pipeline = Pipeline([
    ('selector', DataFrameSelector(feature_attribs)),
    ('imputer', SimpleImputer(strategy="median")),
#     ('min_max_scaler', MinMaxScaler())
])


X_train = features_pipeline.fit_transform(train_set)


X_test = features_pipeline.fit_transform(test_set)


# 正解値labelを生成するpipeline
label_attrib = ['dov_angle']
facing_dov_angles = [0, 45, 315]


label_pipeline = Pipeline([
    ('selector', DataFrameSelector(label_attrib)),
    ('transducer', ValueTransducer(facing_dov_angles))
])


y_train = label_pipeline.fit_transform(train_set)
y_test = label_pipeline.fit_transform(test_set)


pd.DataFrame(y_train)[:10]


X_train.shape[0]


from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform


# ハイパーパラメータの設定
param_distribs_paceting = {
    'n_estimators': randint(low=1, high=800),
    'min_samples_split': randint(low=2, high=10),
    'min_samples_leaf': randint(low=1, high=10),
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [False],
    'n_jobs': [-1],
    'random_state': [42],
    'max_samples': uniform()
}


# ペースティングでの探索
ext_clf_paceting = ExtraTreesClassifier()
ext_rnd_search_paceting = RandomizedSearchCV(ext_clf_paceting, param_distributions=param_distribs_paceting, n_iter=100, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)


ext_rnd_search_paceting.fit(X_train, y_train)


# ハイパーパラメータの組み合わせを確認する
cvres_paceting = ext_rnd_search_paceting.cv_results_
pd.DataFrame(cvres_paceting)[:10]


# 最良の分類器のハイパーパラメータを確認する
ext_rnd_search_paceting.best_params_


ext_rnd_search_paceting.best_score_


# モデルの保存
ext_best_estimator_paceting = ext_rnd_search_paceting.best_estimator_
joblib.dump(ext_best_estimator_paceting, './pkl/ext_best_estimator_paceting_211007_1.pkl')


# ハイパーパラメータの設定
param_distribs_bagging = {
    'n_estimators': randint(low=100, high=800),
    'min_samples_split': randint(low=2, high=10),
    'min_samples_leaf': randint(low=1, high=10),
    'max_features': ['sqrt', 'log2', None],
    'bootstrap': [True],
    'oob_score': [True, False],
    'n_jobs': [-1],
    'random_state': [42],
    'max_samples': uniform()
}


# バギングでの探索
ext_clf_bagging = ExtraTreesClassifier()
ext_rnd_search_bagging = RandomizedSearchCV(ext_clf_bagging, param_distributions=param_distribs_bagging, n_iter=100, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)
ext_rnd_search_bagging.fit(X_train, y_train)


ext_rnd_search_bagging.best_params_


ext_rnd_search_bagging.best_score_


ext_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)


ext_clf.fit(X_train, y_train)


feature_score = {}
for name, score in zip(feature_attribs, ext_clf.feature_importances_):
    feature_score[name] = score
    
# sort
feature_score_sorted = sorted(feature_score.items(), key=lambda x:x[1], reverse=True)
pprint.pprint(feature_score_sorted)


y_pred_ext = ext_clf.predict(X_test)


ext_clf.score(X_train, y_train)


cross_val_score(ext_clf, X_train, y_train, cv=3, scoring='accuracy')


# K分割交差検証を行い、個々のテストフォールドに対する予測結果を返す。（評価のスコアではない）
y_train_pred = cross_val_predict(ext_clf, X_train, y_train, cv=3)
y_train_pred


# 混同行列
confusion_matrix(y_train, y_train_pred)


# F1スコアの計算
f1_score(y_train, y_train_pred)


ext_clf.score(X_test, y_test)


<<<<<<< HEAD

=======
import joblib


model = joblib.load
>>>>>>> aba3818315f78696fb0a0e0f21384547f3e2a2de
