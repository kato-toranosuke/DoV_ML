import numpy as np
import pandas as pd
from typing import List, Any, Union, Dict
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
import pprint
from sklearn.ensemble import ExtraTreesClassifier
import joblib
from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate
from sklearn.metrics import confusion_matrix, f1_score
from sklearn.ensemble import RandomForestClassifier
import os, re


CSV_PATH = "../out/csv/"


def CsvToDf(csv_list: List) -> pd.DataFrame:
    df = pd.concat([pd.read_csv(CSV_PATH + filename) for filename in csv_list])
    return df


# 読み込むCSVファイルのリスト
csv_list = ["fearures_mono_ch_complete_211007.csv"]
df = CsvToDf(csv_list)


# session1のデータの抽出(式を評価するengineとしてnumexprを使用することで、処理の高速化を狙う。)
train_set = df.query('session_id == "trial1"', engine='numexpr')
test_set = df.query('session_id == "trial2"', engine='numexpr')


class DataFrameSelector(BaseEstimator, TransformerMixin):
    '''
    DataFrameから、属性を選択し、Numpy Arrayへ変換する変換器。
    '''

    def __init__(self, attribute_names: List) -> None:
        self.attribute_names = attribute_names

    def fit(self, X: pd.DataFrame, y=None):
        return self

    def transform(self, X: pd.DataFrame) -> np.ndarray:
        return X[self.attribute_names].to_numpy()


class ValueTransducer(BaseEstimator, TransformerMixin):
    '''
    値を他の値に変換する。
    '''

    def __init__(self, vals) -> None:
        self.vals = vals

    def fit(self, X: np.ndarray, y=None):
        return self

    def transform(self, X: np.ndarray) -> np.ndarray:
        X_ = []
        # Trueになる値->1, それ以外->0
        for val in self.vals:
            x_ = np.where(X == val, 1, 0)
            X_.append(x_)
        # ndarrayに変換
        X_ = np.array(X_)

        # 1次元配列にまとめる
        label_list = []
        for i in range(len(X)):
            bool = X_[:, i].any()
            label_list.append(int(bool))
        # ndarrayに変換
        label_np = np.array(label_list)

        return label_np


# 特徴量に該当する列の抽出
feature_attribs = ['low_power', 'high_power', 'hlbr', 'coe1[0]',	'coe1[1]', 'coe3[0]', 'coe3[1]', 'coe3[2]',	'coe3[3]', 'ratio_max_to_10ms_ave_peaks', 'ratio_max_to_9th_ave_peaks', 'ac_std', 'ac_auc', 'diff_std', 'diff_auc',	'srmr', 'gp_max_val_std', 'gp_max_val_range',
                       'gp_max_val_min', 'gp_max_val_max', 'gp_max_val_mean', 'gp_max_ix_std', 'gp_max_ix_range', 'gp_max_ix_min', 'gp_max_ix_max', 'gp_max_ix_mean', 'gp_auc_std', 'gp_auc_range', 'gp_auc_min', 'gp_auc_max', 'gp_auc_mean',	'tdoa_std', 'tdoa_range', 'tdoa_min', 'tdoa_max', 'tdoa_mean']


# pipeline
features_pipeline = Pipeline([
    ('selector', DataFrameSelector(feature_attribs)),
    ('imputer', SimpleImputer(strategy="median")),
#     ('min_max_scaler', MinMaxScaler())
])


X_train = features_pipeline.fit_transform(train_set)


X_test = features_pipeline.fit_transform(test_set)


# 正解値labelを生成するpipeline
label_attrib = ['dov_angle']
facing_dov_angles = [0, 45, 315]


label_pipeline = Pipeline([
    ('selector', DataFrameSelector(label_attrib)),
    ('transducer', ValueTransducer(facing_dov_angles))
])


y_train = label_pipeline.fit_transform(train_set)
y_test = label_pipeline.fit_transform(test_set)


# 交差検証の結果を表示する
def display_scores(scores):
    print("Scores: ", scores)
    print("Mean: ", scores.mean())
    print("Standard deviation: ", scores.std())


from sklearn.model_selection import GridSearchCV


param_grid = [
    {'n_estimators': [2, 10, 100, 1000], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10], 'max_features': ['sqrt', 'log2', None], 'bootstrap': [False], 'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.01, 0.5, 0.09]},
    {'n_estimators': [2, 10, 100, 1000], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10], 'max_features': ['sqrt', 'log2', None], 'bootstrap': [True], 'oob_score': [True, False], 'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.01, 0.5, 0.09]}
]


param_grid = [
    {'n_estimators': [10], 'min_samples_split': [2], 'min_samples_leaf': [5], 'max_features': [None], 'bootstrap': [False], 'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.5]}
]


ext_clf = ExtraTreesClassifier()
ext_grid_search = GridSearchCV(estimator=ext_clf, param_grid=param_grid, scoring=['accuracy', 'balanced_accuracy', 'f1'], n_jobs=-1, refit='accuracy', cv=5, return_train_score=True)


ext_clf.__class__.__name__


ext_grid_search.fit(X_train, y_train)


cvres_grid_search = ext_grid_search.cv_results_
pd.DataFrame(cvres_grid_search)[:10]


ext_grid_search.best_params_


ext_grid_search.best_score_


# モデルの保存
joblib.dump(ext_grid_search.best_estimator_, './pkl/ext_best_estimator_gridsearch_211008_1.pkl')


best_ext_clf = ext_grid_search.best_estimator_


best_ext_clf.__class__


best_ext_clf.score(X_test, y_test)


# K分割交差検証
y_test_pred = cross_val_predict(best_ext_clf, X_test, y_test, cv=3)
res = confusion_matrix(y_test, y_test_pred)


f1_score(y_test, y_test_pred)


# cross_validation
scoring = ['accuracy', 'balanced_accuracy', 'f1']
scores = cross_validate(best_ext_clf, X_test, y_test, scoring = scoring, cv=10, n_jobs=-1)


scores


np.mean(scores['test_accuracy'])


# for ext_best_estimator_gridsearch_211012_1.pkl
param_grid = [
    {'n_estimators': [5, 10, 20, 30, 40, 50, 60], 'min_samples_split': [2, 3, 4], 'min_samples_leaf': [3, 5, 8], 'max_features': [None], 'bootstrap': [True], 'oob_score': [True],'n_jobs': [-1], 'random_state': [42], 'max_samples': [None, 0.09]}
]

# best params
# {'bootstrap': True,
#  'max_features': None,
#  'max_samples': 0.09,
#  'min_samples_leaf': 5,
#  'min_samples_split': 2,
#  'n_estimators': 10,
#  'n_jobs': -1,
#  'oob_score': True,
#  'random_state': 42}


# for ext_best_estimator_gridsearch_211012_2.pkl
param_grid = [
    {'n_estimators': [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'min_samples_split': [2], 'min_samples_leaf': [4, 5, 6, 7], 'max_features': [None], 'bootstrap': [True], 'oob_score': [True],'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.09]}
]

# best params
# {'bootstrap': True,
#  'max_features': None,
#  'max_samples': 0.09,
#  'min_samples_leaf': 5,
#  'min_samples_split': 2,
#  'n_estimators': 7,
#  'n_jobs': -1,
#  'oob_score': True,
#  'random_state': 42}

# result
# 0.7395833333333334
# array([[2707,  893],
#       [ 905, 1255]])
# f1: 0.5826369545032497


ext_clf = ExtraTreesClassifier()
ext_grid_search = GridSearchCV(estimator=ext_clf, param_grid=param_grid, scoring=['accuracy', 'balanced_accuracy', 'f1'], n_jobs=-1, refit='accuracy', cv=5)
ext_grid_search.fit(X_train, y_train)


ext_grid_search.best_params_


joblib.dump(ext_grid_search.best_estimator_, './pkl/ext_best_estimator_gridsearch_211012_2.pkl')


ext_clf = ext_grid_search.best_estimator_


ext_clf.__dict__


ext_clf.score(X_test, y_test)


y_test_pred = cross_val_predict(ext_clf, X_test, y_test, cv=3)
confusion_matrix(y_test, y_test_pred)


# 交差検証
scores = cross_val_score(ext_clf, X_test, y_test, scoring="accuracy", cv=10)
display_scores(scores)


# 交差検証
scores = cross_val_score(ext_clf, X_test, y_test, scoring="accuracy", cv=3)
display_scores(scores)


f1_score(y_test, y_test_pred)


param_grid = [
    {'n_estimators': [2, 10, 100, 1000], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10], 'max_features': ['sqrt', 'log2', None], 'bootstrap': [False], 'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.01, 0.5, 0.09]},
    {'n_estimators': [2, 10, 100, 1000], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10], 'max_features': ['sqrt', 'log2', None], 'bootstrap': [True], 'oob_score': [True, False], 'n_jobs': [-1], 'random_state': [42], 'max_samples': [0.01, 0.5, 0.09]}
]


rnd_clf = RandomForestClassifier()
rnd_grid_search = GridSearchCV(estimator=rnd_clf, param_grid=param_grid, scoring=['accuracy', 'balanced_accuracy', 'f1'], n_jobs=-1, refit='accuracy', cv=5, return_train_score=True)


rnd_grid_search.fit(X_train, y_train)


def RecFeaturesImportance(model, feature_attrbs):
    feature_score = {}
    for name, score in zip(feature_attrbs, model.feature_importances_):
        feature_score[name] = score
    
    # sort
    feature_score_sorted = sorted(feature_score.items(), key=lambda x:x[1], reverse=True)
    
    return feature_score_sorted


importance = RecFeaturesImportance(best_ext_clf, feature_attribs)


best_ext_clf.feature_importances_


best_ext_clf.n_features_in_


best_ext_clf.n_features_


f_dict = {}
for i in importance:
    print(i[0])
    f_dict[i[0]] = i[1]


for f in f_dict:
    print(f)


import datetime

d_today = datetime.date.today()
d_today.isoweekday()


d_today.isocalendar()


d_today.isoformat()


d_today.timetuple()


type(datetime.date.today().isoformat())


dirs_list = os.listdir('../out/')
dirs_list


no = 0
val = "features"
regex = re.compile(r'^' + re.escape(val) + r'.*')
for dir_name in dirs_list:
    #is_match = re.match(r'^features.*', dir_name)
    #is_match = re.match(r'features.*', dir_name)
    is_match = regex.match(dir_name)
    if is_match != None:
        no += 1
        
no


X = [1, 2, 3]

with open('test.md', 'w') as f:
    string = ['X: ', str(X), '\n']
    f.writelines(string)
    string2 = 'X: ' + str(X)
    f.writelines(string2)


class Const():
  def __init__(self, CSV_PATH="../out/csv/", OUTPUT_PATH="./result/", FEATURE_ATTRBS=['low_power', 'high_power', 'hlbr', 'coe1[0]',	'coe1[1]', 'coe3[0]', 'coe3[1]', 'coe3[2]',	'coe3[3]', 'ratio_max_to_10ms_ave_peaks', 'ratio_max_to_9th_ave_peaks', 'ac_std', 'ac_auc', 'diff_std', 'diff_auc',	'srmr', 'gp_max_val_std', 'gp_max_val_range', 'gp_max_val_min', 'gp_max_val_max', 'gp_max_val_mean', 'gp_max_ix_std', 'gp_max_ix_range', 'gp_max_ix_min', 'gp_max_ix_max', 'gp_max_ix_mean', 'gp_auc_std', 'gp_auc_range', 'gp_auc_min', 'gp_auc_max', 'gp_auc_mean',	'tdoa_std', 'tdoa_range', 'tdoa_min', 'tdoa_max', 'tdoa_mean'], LABEL_ATTRB=['dov_angle'], FACING_DOV_ANGLES=[0, 45, 315], NCV=10):
    # インスタンス変数
    self.CSV_PATH = CSV_PATH
    self.OUTPUT_PATH = OUTPUT_PATH
    self.FEATURE_ATTRBS = FEATURE_ATTRBS
    self.LABEL_ATTRB = LABEL_ATTRB
    self.FACING_DOV_ANGLES = FACING_DOV_ANGLES
    self.NCV = NCV


const = Const()


const.__dict__


const.__class__.__name__


s = "# hoge\n"
s += '- hoge\n'
s += '\t- fuga\n'
print(s)


s = '\t' * 0
s += 'hoge'
print(s)


from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids


class ML_Consts():
  '''
  機械学習に用いる定数を定義するクラス
  '''

  def __init__(self, csv_path="../../out/csv/", output_path="./result/", feature_attrbs=['low_power', 'high_power', 'hlbr', 'coe1[0]',	'coe1[1]', 'coe3[0]', 'coe3[1]', 'coe3[2]',	'coe3[3]', 'ratio_max_to_10ms_ave_peaks', 'ratio_max_to_9th_ave_peaks', 'ac_std', 'ac_auc', 'diff_std', 'diff_auc',	'srmr', 'gp_max_val_std', 'gp_max_val_range', 'gp_max_val_min', 'gp_max_val_max', 'gp_max_val_mean', 'gp_max_ix_std', 'gp_max_ix_range', 'gp_max_ix_min', 'gp_max_ix_max', 'gp_max_ix_mean', 'gp_auc_std', 'gp_auc_range', 'gp_auc_min', 'gp_auc_max', 'gp_auc_mean',	'tdoa_std', 'tdoa_range', 'tdoa_min', 'tdoa_max', 'tdoa_mean'], label_attrb=['dov_angle'], facing_dov_angles=[0, 45, 315], ncv=10, scoring=['accuracy', 'balanced_accuracy', 'f1'], refit_scoring='balanced_accuracy', param_grid=None):
    # インスタンス変数
    self.CSV_PATH = csv_path
    self.OUTPUT_PATH = output_path
    self.FEATURE_ATTRBS = feature_attrbs
    self.LABEL_ATTRB = label_attrb
    self.FACING_DOV_ANGLES = facing_dov_angles
    self.NCV = ncv
    self.SCORING = scoring
    self.REFIT_SCORING = refit_scoring
    self.PARAM_GRID = param_grid


param_grid = [
    {'est__n_estimators': [10], 'est__min_samples_split': [2], 'est__min_samples_leaf': [5], 'est__max_features': [None], 'est__bootstrap': [False], 'est__n_jobs': [-1], 'est__random_state': [42], 'est__max_samples': [0.5]}
]

consts = ML_Consts(param_grid=param_grid)


estimator = ExtraTreesClassifier()
resampler = ClusterCentroids(random_state=42)
train_pipeline = ImbPipeline(steps=[('resmp', resampler), ('est', estimator)])


search = GridSearchCV(estimator=train_pipeline, param_grid=consts.PARAM_GRID, scoring=consts.SCORING, n_jobs=-1, refit=consts.REFIT_SCORING, cv=consts.NCV, return_train_score=False)
search.fit(X_train, y_train)


search.best_estimator_['resmp']


search.best_estimator_[1].__class__.__name__



